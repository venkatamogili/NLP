{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatamogili/NLP/blob/main/Differentiating_LSTM_and_RNN_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDzjUUEWmflW",
        "outputId": "d2ebe12d-44a7-4b8a-f3d1-2d2f733fa125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 74\n",
            "Max sequence length: 17\n",
            "Training samples: 10\n",
            "\n",
            "Training RNN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final RNN Accuracy: 1.0000\n",
            "\n",
            "Training LSTM...\n",
            "Final LSTM Accuracy: 1.0000\n",
            "\n",
            "============================================================\n",
            "PREDICTIONS\n",
            "============================================================\n",
            "\n",
            "Test: 'The movie started off slow and confusing, but ended beautifully.'\n",
            "  RNN:  Positive (confidence: 0.6361)\n",
            "  LSTM: Negative (confidence: 0.2233)\n",
            "\n",
            "Test: 'Absolutely terrible from beginning to end.'\n",
            "  RNN:  Negative (confidence: 0.2130)\n",
            "  LSTM: Negative (confidence: 0.1890)\n",
            "\n",
            "Test: 'A stunning masterpiece with incredible performances.'\n",
            "  RNN:  Positive (confidence: 0.5855)\n",
            "  LSTM: Positive (confidence: 0.9898)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set Seeds for Reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Enhanced Dataset with more examples\n",
        "sentences = [\n",
        "    \"The beginning of the movie was slow and boring, yet the final scenes were thrilling and exciting.\",\n",
        "    \"While the film had great visuals, the storyline lacked depth and left the audience disappointed.\",\n",
        "    \"The acting was phenomenal, and the movie had an emotional ending that left everyone in tears.\",\n",
        "    \"Absolutely terrible movie with no redeeming qualities whatsoever.\",\n",
        "    \"A masterpiece of cinema that everyone should watch.\",\n",
        "    \"The plot was confusing and the pacing was awful.\",\n",
        "    \"Brilliant performances and stunning cinematography throughout.\",\n",
        "    \"Started strong but fell apart in the second half.\",\n",
        "    \"An emotional rollercoaster that ends on a high note.\",\n",
        "    \"Boring and predictable from start to finish.\"\n",
        "]\n",
        "\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Padding\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "X = np.array(padded_sequences, dtype=np.int32)\n",
        "y = np.array(labels, dtype=np.int32)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Max sequence length: {max_len}\")\n",
        "print(f\"Training samples: {len(X)}\")\n",
        "\n",
        "# Improved RNN Model with Dropout\n",
        "rnn_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "    SimpleRNN(32, activation='tanh', return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Improved LSTM Model with Dropout\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "    LSTM(32, activation='tanh', return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train with more epochs\n",
        "print(\"\\nTraining RNN...\")\n",
        "rnn_history = rnn_model.fit(X, y, epochs=50, verbose=0, validation_split=0.2)\n",
        "print(f\"Final RNN Accuracy: {rnn_history.history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "print(\"\\nTraining LSTM...\")\n",
        "lstm_history = lstm_model.fit(X, y, epochs=50, verbose=0, validation_split=0.2)\n",
        "print(f\"Final LSTM Accuracy: {lstm_history.history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "# Test multiple sentences\n",
        "test_sentences = [\n",
        "    \"The movie started off slow and confusing, but ended beautifully.\",\n",
        "    \"Absolutely terrible from beginning to end.\",\n",
        "    \"A stunning masterpiece with incredible performances.\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for test_sentence in test_sentences:\n",
        "    print(f\"\\nTest: '{test_sentence}'\")\n",
        "\n",
        "    test_seq = tokenizer.texts_to_sequences([test_sentence])\n",
        "    test_seq = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    rnn_pred = rnn_model.predict(test_seq, verbose=0)\n",
        "    rnn_sentiment = \"Positive\" if rnn_pred[0][0] > 0.5 else \"Negative\"\n",
        "\n",
        "    lstm_pred = lstm_model.predict(test_seq, verbose=0)\n",
        "    lstm_sentiment = \"Positive\" if lstm_pred[0][0] > 0.5 else \"Negative\"\n",
        "\n",
        "    print(f\"  RNN:  {rnn_sentiment:8s} (confidence: {rnn_pred[0][0]:.4f})\")\n",
        "    print(f\"  LSTM: {lstm_sentiment:8s} (confidence: {lstm_pred[0][0]:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set Seeds\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Generate larger synthetic dataset\n",
        "positive_templates = [\n",
        "    \"The movie was {adj1} and {adj2}, with {feature} that {verb}.\",\n",
        "    \"Absolutely {adj1} film with {adj2} {feature}.\",\n",
        "    \"A {adj1} masterpiece that {verb} the audience.\",\n",
        "    \"{feature} was {adj1} and the ending {verb}.\",\n",
        "    \"Brilliant {feature} and {adj2} performances throughout.\"\n",
        "]\n",
        "\n",
        "negative_templates = [\n",
        "    \"The movie was {adj1} and {adj2}, with {feature} that {verb}.\",\n",
        "    \"Absolutely {adj1} film with {adj2} {feature}.\",\n",
        "    \"The {feature} was {adj1} and {adj2}.\",\n",
        "    \"{adj1} and {adj2} from start to finish.\",\n",
        "    \"Terrible {feature} that {verb} the audience.\"\n",
        "]\n",
        "\n",
        "positive_words = {\n",
        "    'adj1': ['amazing', 'brilliant', 'fantastic', 'wonderful', 'outstanding', 'spectacular', 'excellent', 'superb', 'incredible', 'marvelous'],\n",
        "    'adj2': ['engaging', 'thrilling', 'beautiful', 'powerful', 'moving', 'captivating', 'stunning', 'emotional', 'heartwarming', 'uplifting'],\n",
        "    'feature': ['cinematography', 'acting', 'storyline', 'directing', 'soundtrack', 'script', 'visuals', 'performances', 'plot', 'dialogue'],\n",
        "    'verb': ['captivated', 'amazed', 'moved', 'impressed', 'touched', 'inspired', 'delighted', 'entertained', 'mesmerized', 'thrilled']\n",
        "}\n",
        "\n",
        "negative_words = {\n",
        "    'adj1': ['terrible', 'awful', 'boring', 'disappointing', 'dreadful', 'horrible', 'poor', 'weak', 'mediocre', 'uninspired'],\n",
        "    'adj2': ['confusing', 'slow', 'predictable', 'shallow', 'tedious', 'lifeless', 'bland', 'lackluster', 'forgettable', 'clichéd'],\n",
        "    'feature': ['cinematography', 'acting', 'storyline', 'directing', 'soundtrack', 'script', 'visuals', 'performances', 'plot', 'dialogue'],\n",
        "    'verb': ['disappointed', 'bored', 'confused', 'frustrated', 'annoyed', 'let down', 'failed', 'alienated', 'exhausted', 'irritated']\n",
        "}\n",
        "\n",
        "def generate_sentences(templates, word_dict, n=500):\n",
        "    sentences = []\n",
        "    for _ in range(n):\n",
        "        template = random.choice(templates)\n",
        "        sentence = template.format(\n",
        "            adj1=random.choice(word_dict['adj1']),\n",
        "            adj2=random.choice(word_dict['adj2']),\n",
        "            feature=random.choice(word_dict['feature']),\n",
        "            verb=random.choice(word_dict['verb'])\n",
        "        )\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "# Generate 1000 positive and 1000 negative examples\n",
        "positive_sentences = generate_sentences(positive_templates, positive_words, 1000)\n",
        "negative_sentences = generate_sentences(negative_templates, negative_words, 1000)\n",
        "\n",
        "sentences = positive_sentences + negative_sentences\n",
        "labels = [1] * 1000 + [0] * 1000\n",
        "\n",
        "# Shuffle\n",
        "combined = list(zip(sentences, labels))\n",
        "random.shuffle(combined)\n",
        "sentences, labels = zip(*combined)\n",
        "\n",
        "print(f\"Total training samples: {len(sentences)}\")\n",
        "print(f\"Sample positive: {sentences[0]}\")\n",
        "print(f\"Sample negative: {sentences[-1]}\")\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "max_len = 30  # Fixed length for efficiency\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "X = np.array(padded_sequences, dtype=np.int32)\n",
        "y = np.array(labels, dtype=np.int32)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Train/Test Split\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(f\"\\nVocabulary size: {vocab_size}\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "\n",
        "# RNN Model\n",
        "rnn_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "    SimpleRNN(32, activation='tanh'),\n",
        "    Dropout(0.5),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "    LSTM(32, activation='tanh'),\n",
        "    Dropout(0.5),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train RNN\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training RNN...\")\n",
        "print(\"=\"*60)\n",
        "rnn_history = rnn_model.fit(X_train, y_train, epochs=10, batch_size=32,\n",
        "                             validation_split=0.2, verbose=1)\n",
        "\n",
        "# Train LSTM\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training LSTM...\")\n",
        "print(\"=\"*60)\n",
        "lstm_history = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32,\n",
        "                               validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rnn_loss, rnn_acc = rnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "lstm_loss, lstm_acc = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nRNN  - Test Accuracy: {rnn_acc:.4f}, Test Loss: {rnn_loss:.4f}\")\n",
        "print(f\"LSTM - Test Accuracy: {lstm_acc:.4f}, Test Loss: {lstm_loss:.4f}\")\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"The movie was brilliant and captivating with amazing performances.\",\n",
        "    \"Absolutely terrible film with boring and confusing plot.\",\n",
        "    \"The acting was wonderful and the soundtrack was beautiful.\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for test_sentence in test_sentences:\n",
        "    test_seq = tokenizer.texts_to_sequences([test_sentence])\n",
        "    test_seq = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    rnn_pred = rnn_model.predict(test_seq, verbose=0)\n",
        "    lstm_pred = lstm_model.predict(test_seq, verbose=0)\n",
        "\n",
        "    print(f\"\\nTest: '{test_sentence}'\")\n",
        "    print(f\"  RNN:  {'Positive' if rnn_pred[0][0] > 0.5 else 'Negative':8s} (confidence: {rnn_pred[0][0]:.4f})\")\n",
        "    print(f\"  LSTM: {'Positive' if lstm_pred[0][0] > 0.5 else 'Negative':8s} (confidence: {lstm_pred[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIBY8nNIFfsa",
        "outputId": "21098a78-7e71-460d-bbd6-8518e68d6255"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training samples: 2000\n",
            "Sample positive: uninspired and tedious from start to finish.\n",
            "Sample negative: The movie was boring and predictable, with cinematography that irritated.\n",
            "\n",
            "Vocabulary size: 90\n",
            "Training samples: 1600\n",
            "Testing samples: 400\n",
            "\n",
            "============================================================\n",
            "Training RNN...\n",
            "============================================================\n",
            "Epoch 1/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.7063 - loss: 0.5796 - val_accuracy: 1.0000 - val_loss: 0.0926\n",
            "Epoch 2/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0789 - val_accuracy: 1.0000 - val_loss: 0.0126\n",
            "Epoch 3/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0162 - val_accuracy: 1.0000 - val_loss: 0.0046\n",
            "Epoch 4/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0080 - val_accuracy: 1.0000 - val_loss: 0.0024\n",
            "Epoch 5/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
            "Epoch 6/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 8.9709e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 6.1881e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 4.4690e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 3.2349e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 2.4480e-04\n",
            "\n",
            "============================================================\n",
            "Training LSTM...\n",
            "============================================================\n",
            "Epoch 1/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.4886 - loss: 0.6933 - val_accuracy: 0.4781 - val_loss: 0.6939\n",
            "Epoch 2/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5178 - loss: 0.6933 - val_accuracy: 0.8969 - val_loss: 0.6902\n",
            "Epoch 3/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7491 - loss: 0.5569 - val_accuracy: 1.0000 - val_loss: 0.0208\n",
            "Epoch 4/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0179 - val_accuracy: 1.0000 - val_loss: 0.0020\n",
            "Epoch 5/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 9.8673e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 5.6963e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 3.6882e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 2.5390e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 8.7590e-04 - val_accuracy: 1.0000 - val_loss: 1.8739e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 7.8745e-04 - val_accuracy: 1.0000 - val_loss: 1.4010e-04\n",
            "\n",
            "============================================================\n",
            "EVALUATION ON TEST SET\n",
            "============================================================\n",
            "\n",
            "RNN  - Test Accuracy: 1.0000, Test Loss: 0.0002\n",
            "LSTM - Test Accuracy: 1.0000, Test Loss: 0.0001\n",
            "\n",
            "============================================================\n",
            "PREDICTIONS\n",
            "============================================================\n",
            "\n",
            "Test: 'The movie was brilliant and captivating with amazing performances.'\n",
            "  RNN:  Positive (confidence: 0.9996)\n",
            "  LSTM: Positive (confidence: 0.9999)\n",
            "\n",
            "Test: 'Absolutely terrible film with boring and confusing plot.'\n",
            "  RNN:  Negative (confidence: 0.0002)\n",
            "  LSTM: Negative (confidence: 0.0001)\n",
            "\n",
            "Test: 'The acting was wonderful and the soundtrack was beautiful.'\n",
            "  RNN:  Positive (confidence: 0.9903)\n",
            "  LSTM: Positive (confidence: 0.9999)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set Seeds\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Create dataset with LONG-RANGE DEPENDENCIES and NEGATIONS\n",
        "# This will challenge RNN but LSTM should handle better\n",
        "\n",
        "def generate_complex_sentences(n=1000):\n",
        "    \"\"\"\n",
        "    Generate sentences where sentiment depends on words far apart in the sequence.\n",
        "    Uses negations, contrasts, and long-distance dependencies.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    # Positive starts, negative endings (label: 0 - Negative overall)\n",
        "    neg_templates = [\n",
        "        \"The movie started with {pos1} scenes and {pos2} acting, however after the first hour it became {neg1} and {neg2}, ultimately leaving me {neg3}.\",\n",
        "        \"Initially the film seemed {pos1} with {pos2} cinematography, but unfortunately the second half was {neg1} and the ending was {neg2}, making it {neg3} overall.\",\n",
        "        \"Despite having {pos1} visuals and {pos2} music at the beginning, the movie deteriorated into a {neg1} mess with {neg2} pacing and {neg3} conclusion.\",\n",
        "        \"The opening was {pos1} and the characters seemed {pos2}, yet as the story progressed everything became {neg1} and {neg2}, resulting in a {neg3} experience.\",\n",
        "        \"Although it began with {pos1} promise and {pos2} performances, the film sadly turned {neg1} with a {neg2} plot and {neg3} ending.\",\n",
        "    ]\n",
        "\n",
        "    # Negative starts, positive endings (label: 1 - Positive overall)\n",
        "    pos_templates = [\n",
        "        \"The movie started with {neg1} pacing and {neg2} dialogue, however the final act was absolutely {pos1} and {pos2}, leaving me thoroughly {pos3}.\",\n",
        "        \"Initially the film was {neg1} with {neg2} acting, but thankfully the second half became {pos1} and the conclusion was {pos2}, making it {pos3} overall.\",\n",
        "        \"Despite having {neg1} beginning and {neg2} characters early on, the movie transformed into a {pos1} masterpiece with {pos2} emotional depth and {pos3} finale.\",\n",
        "        \"The opening was {neg1} and the setup seemed {neg2}, yet as the story unfolded everything became {pos1} and {pos2}, resulting in a {pos3} experience.\",\n",
        "        \"Although it began with {neg1} scenes and {neg2} writing, the film remarkably became {pos1} with a {pos2} climax and {pos3} ending.\",\n",
        "    ]\n",
        "\n",
        "    positive_words = {\n",
        "        'pos1': ['brilliant', 'amazing', 'fantastic', 'wonderful', 'stunning', 'excellent', 'superb', 'outstanding', 'remarkable', 'incredible'],\n",
        "        'pos2': ['captivating', 'engaging', 'beautiful', 'powerful', 'moving', 'thrilling', 'emotional', 'touching', 'inspiring', 'uplifting'],\n",
        "        'pos3': ['satisfied', 'impressed', 'delighted', 'moved', 'entertained', 'inspired', 'amazed', 'touched', 'fulfilled', 'overjoyed']\n",
        "    }\n",
        "\n",
        "    negative_words = {\n",
        "        'neg1': ['boring', 'terrible', 'awful', 'disappointing', 'dull', 'weak', 'poor', 'mediocre', 'tedious', 'lackluster'],\n",
        "        'neg2': ['confusing', 'frustrating', 'predictable', 'slow', 'shallow', 'lifeless', 'bland', 'uninspired', 'clichéd', 'forgettable'],\n",
        "        'neg3': ['disappointed', 'unsatisfied', 'frustrated', 'bored', 'let down', 'annoyed', 'dissatisfied', 'underwhelmed', 'regretful', 'dismayed']\n",
        "    }\n",
        "\n",
        "    # Generate negative overall (starts positive, ends negative)\n",
        "    for _ in range(n // 2):\n",
        "        template = random.choice(neg_templates)\n",
        "        sentence = template.format(\n",
        "            pos1=random.choice(positive_words['pos1']),\n",
        "            pos2=random.choice(positive_words['pos2']),\n",
        "            neg1=random.choice(negative_words['neg1']),\n",
        "            neg2=random.choice(negative_words['neg2']),\n",
        "            neg3=random.choice(negative_words['neg3'])\n",
        "        )\n",
        "        sentences.append(sentence)\n",
        "        labels.append(0)  # Negative overall\n",
        "\n",
        "    # Generate positive overall (starts negative, ends positive)\n",
        "    for _ in range(n // 2):\n",
        "        template = random.choice(pos_templates)\n",
        "        sentence = template.format(\n",
        "            neg1=random.choice(negative_words['neg1']),\n",
        "            neg2=random.choice(negative_words['neg2']),\n",
        "            pos1=random.choice(positive_words['pos1']),\n",
        "            pos2=random.choice(positive_words['pos2']),\n",
        "            pos3=random.choice(positive_words['pos3'])\n",
        "        )\n",
        "        sentences.append(sentence)\n",
        "        labels.append(1)  # Positive overall\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# Generate 2000 samples with long-range dependencies\n",
        "sentences, labels = generate_complex_sentences(2000)\n",
        "\n",
        "# Shuffle\n",
        "combined = list(zip(sentences, labels))\n",
        "random.shuffle(combined)\n",
        "sentences, labels = zip(*combined)\n",
        "\n",
        "print(f\"Total samples: {len(sentences)}\")\n",
        "print(f\"\\nExample NEGATIVE (starts positive, ends negative):\")\n",
        "neg_example = [s for s, l in zip(sentences[:10], labels[:10]) if l == 0][0]\n",
        "print(f\"{neg_example}\\n\")\n",
        "print(f\"Example POSITIVE (starts negative, ends positive):\")\n",
        "pos_example = [s for s, l in zip(sentences[:10], labels[:10]) if l == 1][0]\n",
        "print(f\"{pos_example}\\n\")\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Use longer max_len to preserve the full context\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "X = np.array(padded_sequences, dtype=np.int32)\n",
        "y = np.array(labels, dtype=np.int32)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Train/Test Split (80/20)\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "\n",
        "# RNN Model (will struggle with long sequences)\n",
        "rnn_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "    SimpleRNN(32, activation='tanh'),  # No return_sequences - only final state\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# LSTM Model (should handle long-range dependencies better)\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "    LSTM(32, activation='tanh'),  # Gates help maintain long-term memory\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train RNN\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training RNN...\")\n",
        "print(\"=\"*60)\n",
        "rnn_history = rnn_model.fit(X_train, y_train, epochs=15, batch_size=32,\n",
        "                             validation_split=0.2, verbose=1)\n",
        "\n",
        "# Train LSTM\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training LSTM...\")\n",
        "print(\"=\"*60)\n",
        "lstm_history = lstm_model.fit(X_train, y_train, epochs=15, batch_size=32,\n",
        "                               validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rnn_loss, rnn_acc = rnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "lstm_loss, lstm_acc = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nRNN  - Test Accuracy: {rnn_acc:.4f}, Test Loss: {rnn_loss:.4f}\")\n",
        "print(f\"LSTM - Test Accuracy: {lstm_acc:.4f}, Test Loss: {lstm_loss:.4f}\")\n",
        "print(f\"\\nPerformance Gap: {(lstm_acc - rnn_acc) * 100:.2f}% better with LSTM\")\n",
        "\n",
        "# Test sentences with long-range dependencies\n",
        "test_sentences = [\n",
        "    \"The movie started with brilliant scenes and amazing acting, however after the first hour it became boring and terrible, ultimately leaving me disappointed.\",\n",
        "    \"Initially the film was terrible with boring dialogue, but thankfully the second half became fantastic and the conclusion was beautiful, making it wonderful overall.\",\n",
        "    \"Despite having stunning visuals at the beginning, the movie deteriorated into a confusing mess with slow pacing.\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTIONS ON COMPLEX SENTENCES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for test_sentence in test_sentences:\n",
        "    test_seq = tokenizer.texts_to_sequences([test_sentence])\n",
        "    test_seq = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    rnn_pred = rnn_model.predict(test_seq, verbose=0)\n",
        "    lstm_pred = lstm_model.predict(test_seq, verbose=0)\n",
        "\n",
        "    print(f\"\\nTest: '{test_sentence[:80]}...'\")\n",
        "    print(f\"  RNN:  {'Positive' if rnn_pred[0][0] > 0.5 else 'Negative':8s} (confidence: {rnn_pred[0][0]:.4f})\")\n",
        "    print(f\"  LSTM: {'Positive' if lstm_pred[0][0] > 0.5 else 'Negative':8s} (confidence: {lstm_pred[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK7x_ZoyF_Dq",
        "outputId": "6a3422c2-aa9c-437c-c3e4-964e49793152"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 2000\n",
            "\n",
            "Example NEGATIVE (starts positive, ends negative):\n",
            "Although it began with wonderful promise and powerful performances, the film sadly turned terrible with a shallow plot and dissatisfied ending.\n",
            "\n",
            "Example POSITIVE (starts negative, ends positive):\n",
            "Initially the film was disappointing with frustrating acting, but thankfully the second half became amazing and the conclusion was touching, making it satisfied overall.\n",
            "\n",
            "Maximum sequence length: 25\n",
            "Vocabulary size: 137\n",
            "Training samples: 1600\n",
            "Testing samples: 400\n",
            "\n",
            "============================================================\n",
            "Training RNN...\n",
            "============================================================\n",
            "Epoch 1/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8267 - loss: 0.4741 - val_accuracy: 1.0000 - val_loss: 0.0348\n",
            "Epoch 2/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0291 - val_accuracy: 1.0000 - val_loss: 0.0057\n",
            "Epoch 3/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 0.0025\n",
            "Epoch 4/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
            "Epoch 5/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 9.7473e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 6.8189e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 5.0632e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 1.0000 - val_loss: 3.7819e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 7.7907e-04 - val_accuracy: 1.0000 - val_loss: 2.9489e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.0376e-04 - val_accuracy: 1.0000 - val_loss: 2.3417e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 5.3303e-04 - val_accuracy: 1.0000 - val_loss: 1.8982e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 4.4159e-04 - val_accuracy: 1.0000 - val_loss: 1.5603e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 3.9365e-04 - val_accuracy: 1.0000 - val_loss: 1.3017e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 3.3327e-04 - val_accuracy: 1.0000 - val_loss: 1.0975e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.3946e-04 - val_accuracy: 1.0000 - val_loss: 9.4118e-05\n",
            "\n",
            "============================================================\n",
            "Training LSTM...\n",
            "============================================================\n",
            "Epoch 1/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7110 - loss: 0.6696 - val_accuracy: 1.0000 - val_loss: 0.2818\n",
            "Epoch 2/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.1394 - val_accuracy: 1.0000 - val_loss: 0.0060\n",
            "Epoch 3/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9998 - loss: 0.0064 - val_accuracy: 0.9656 - val_loss: 0.0856\n",
            "Epoch 4/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9975 - loss: 0.0109 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
            "Epoch 5/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
            "Epoch 6/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 6.8582e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 4.6858e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 8.1189e-04 - val_accuracy: 1.0000 - val_loss: 3.4616e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 6.1651e-04 - val_accuracy: 1.0000 - val_loss: 2.6646e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 4.6382e-04 - val_accuracy: 1.0000 - val_loss: 2.1424e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 4.5481e-04 - val_accuracy: 1.0000 - val_loss: 1.7217e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 3.1706e-04 - val_accuracy: 1.0000 - val_loss: 1.4302e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 2.8253e-04 - val_accuracy: 1.0000 - val_loss: 1.2189e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 2.6236e-04 - val_accuracy: 1.0000 - val_loss: 1.0449e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 2.3183e-04 - val_accuracy: 1.0000 - val_loss: 9.0549e-05\n",
            "\n",
            "============================================================\n",
            "EVALUATION ON TEST SET\n",
            "============================================================\n",
            "\n",
            "RNN  - Test Accuracy: 1.0000, Test Loss: 0.0001\n",
            "LSTM - Test Accuracy: 1.0000, Test Loss: 0.0001\n",
            "\n",
            "Performance Gap: 0.00% better with LSTM\n",
            "\n",
            "============================================================\n",
            "PREDICTIONS ON COMPLEX SENTENCES\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7994001791c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test: 'The movie started with brilliant scenes and amazing acting, however after the fi...'\n",
            "  RNN:  Negative (confidence: 0.0001)\n",
            "  LSTM: Negative (confidence: 0.0000)\n",
            "\n",
            "Test: 'Initially the film was terrible with boring dialogue, but thankfully the second ...'\n",
            "  RNN:  Positive (confidence: 0.9996)\n",
            "  LSTM: Positive (confidence: 0.9998)\n",
            "\n",
            "Test: 'Despite having stunning visuals at the beginning, the movie deteriorated into a ...'\n",
            "  RNN:  Negative (confidence: 0.0007)\n",
            "  LSTM: Negative (confidence: 0.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "# Set Seeds\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Generating 1 million training samples with long-range dependencies...\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "def generate_complex_sentences_batch(n=1000000):\n",
        "    \"\"\"\n",
        "    Generate 1 million sentences where sentiment depends on words far apart.\n",
        "    Optimized for speed using list comprehension and batch processing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Negative overall templates (starts positive, ends negative)\n",
        "    neg_templates = [\n",
        "        \"The movie started with {pos1} scenes and {pos2} acting, however after the first hour it became {neg1} and {neg2}, ultimately leaving me {neg3}.\",\n",
        "        \"Initially the film seemed {pos1} with {pos2} cinematography, but unfortunately the second half was {neg1} and the ending was {neg2}, making it {neg3} overall.\",\n",
        "        \"Despite having {pos1} visuals and {pos2} music at the beginning, the movie deteriorated into a {neg1} mess with {neg2} pacing and {neg3} conclusion.\",\n",
        "        \"The opening was {pos1} and the characters seemed {pos2}, yet as the story progressed everything became {neg1} and {neg2}, resulting in a {neg3} experience.\",\n",
        "        \"Although it began with {pos1} promise and {pos2} performances, the film sadly turned {neg1} with a {neg2} plot and {neg3} ending.\",\n",
        "        \"What started as a {pos1} journey with {pos2} direction quickly devolved into {neg1} chaos with {neg2} writing and a {neg3} finale.\",\n",
        "        \"The film opened with {pos1} energy and {pos2} screenplay, but collapsed into {neg1} tedium with {neg2} execution and {neg3} payoff.\",\n",
        "    ]\n",
        "\n",
        "    # Positive overall templates (starts negative, ends positive)\n",
        "    pos_templates = [\n",
        "        \"The movie started with {neg1} pacing and {neg2} dialogue, however the final act was absolutely {pos1} and {pos2}, leaving me thoroughly {pos3}.\",\n",
        "        \"Initially the film was {neg1} with {neg2} acting, but thankfully the second half became {pos1} and the conclusion was {pos2}, making it {pos3} overall.\",\n",
        "        \"Despite having {neg1} beginning and {neg2} characters early on, the movie transformed into a {pos1} masterpiece with {pos2} emotional depth and {pos3} finale.\",\n",
        "        \"The opening was {neg1} and the setup seemed {neg2}, yet as the story unfolded everything became {pos1} and {pos2}, resulting in a {pos3} experience.\",\n",
        "        \"Although it began with {neg1} scenes and {neg2} writing, the film remarkably became {pos1} with a {pos2} climax and {pos3} ending.\",\n",
        "        \"What started as a {neg1} slog with {neg2} characters eventually evolved into {pos1} brilliance with {pos2} depth and a {pos3} conclusion.\",\n",
        "        \"The film opened with {neg1} confusion and {neg2} pacing, but transformed into {pos1} excellence with {pos2} resolution and {pos3} impact.\",\n",
        "    ]\n",
        "\n",
        "    positive_words = {\n",
        "        'pos1': ['brilliant', 'amazing', 'fantastic', 'wonderful', 'stunning', 'excellent', 'superb', 'outstanding', 'remarkable', 'incredible',\n",
        "                 'spectacular', 'magnificent', 'phenomenal', 'extraordinary', 'breathtaking', 'dazzling', 'exceptional', 'glorious', 'splendid', 'marvelous'],\n",
        "        'pos2': ['captivating', 'engaging', 'beautiful', 'powerful', 'moving', 'thrilling', 'emotional', 'touching', 'inspiring', 'uplifting',\n",
        "                 'compelling', 'gripping', 'mesmerizing', 'enchanting', 'riveting', 'absorbing', 'enthralling', 'stirring', 'poignant', 'evocative'],\n",
        "        'pos3': ['satisfied', 'impressed', 'delighted', 'moved', 'entertained', 'inspired', 'amazed', 'touched', 'fulfilled', 'overjoyed',\n",
        "                 'elated', 'thrilled', 'enchanted', 'captivated', 'mesmerized', 'awestruck', 'gratified', 'pleased', 'content', 'euphoric']\n",
        "    }\n",
        "\n",
        "    negative_words = {\n",
        "        'neg1': ['boring', 'terrible', 'awful', 'disappointing', 'dull', 'weak', 'poor', 'mediocre', 'tedious', 'lackluster',\n",
        "                 'uninspired', 'lifeless', 'dreary', 'monotonous', 'insipid', 'vapid', 'stale', 'flat', 'tiresome', 'wearisome'],\n",
        "        'neg2': ['confusing', 'frustrating', 'predictable', 'slow', 'shallow', 'lifeless', 'bland', 'uninspired', 'clichéd', 'forgettable',\n",
        "                 'convoluted', 'disjointed', 'incoherent', 'muddled', 'derivative', 'hackneyed', 'trite', 'banal', 'formulaic', 'unoriginal'],\n",
        "        'neg3': ['disappointed', 'unsatisfied', 'frustrated', 'bored', 'let down', 'annoyed', 'dissatisfied', 'underwhelmed', 'regretful', 'dismayed',\n",
        "                 'disheartened', 'disillusioned', 'deflated', 'disenchanted', 'dejected', 'dispirited', 'displeased', 'aggrieved', 'vexed', 'irked']\n",
        "    }\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    # Generate in batches for progress tracking\n",
        "    batch_size = 100000\n",
        "    num_batches = n // batch_size\n",
        "\n",
        "    for batch in tqdm(range(num_batches), desc=\"Generating data\"):\n",
        "        batch_sentences = []\n",
        "        batch_labels = []\n",
        "\n",
        "        # Generate negative overall (starts positive, ends negative)\n",
        "        for _ in range(batch_size // 2):\n",
        "            template = random.choice(neg_templates)\n",
        "            sentence = template.format(\n",
        "                pos1=random.choice(positive_words['pos1']),\n",
        "                pos2=random.choice(positive_words['pos2']),\n",
        "                neg1=random.choice(negative_words['neg1']),\n",
        "                neg2=random.choice(negative_words['neg2']),\n",
        "                neg3=random.choice(negative_words['neg3'])\n",
        "            )\n",
        "            batch_sentences.append(sentence)\n",
        "            batch_labels.append(0)\n",
        "\n",
        "        # Generate positive overall (starts negative, ends positive)\n",
        "        for _ in range(batch_size // 2):\n",
        "            template = random.choice(pos_templates)\n",
        "            sentence = template.format(\n",
        "                neg1=random.choice(negative_words['neg1']),\n",
        "                neg2=random.choice(negative_words['neg2']),\n",
        "                pos1=random.choice(positive_words['pos1']),\n",
        "                pos2=random.choice(positive_words['pos2']),\n",
        "                pos3=random.choice(positive_words['pos3'])\n",
        "            )\n",
        "            batch_sentences.append(sentence)\n",
        "            batch_labels.append(1)\n",
        "\n",
        "        sentences.extend(batch_sentences)\n",
        "        labels.extend(batch_labels)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# Generate 1 million samples\n",
        "sentences, labels = generate_complex_sentences_batch(1000000)\n",
        "\n",
        "# Shuffle\n",
        "print(\"\\nShuffling data...\")\n",
        "combined = list(zip(sentences, labels))\n",
        "random.shuffle(combined)\n",
        "sentences, labels = zip(*combined)\n",
        "sentences = list(sentences)\n",
        "labels = list(labels)\n",
        "\n",
        "print(f\"Total samples generated: {len(sentences):,}\")\n",
        "print(f\"\\nExample NEGATIVE (starts positive, ends negative):\")\n",
        "neg_example = [s for s, l in zip(sentences[:100], labels[:100]) if l == 0][0]\n",
        "print(f\"{neg_example}\\n\")\n",
        "print(f\"Example POSITIVE (starts negative, ends positive):\")\n",
        "pos_example = [s for s, l in zip(sentences[:100], labels[:100]) if l == 1][0]\n",
        "print(f\"{pos_example}\\n\")\n",
        "\n",
        "# Tokenization\n",
        "print(\"Tokenizing sentences...\")\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Use fixed max_len for efficiency\n",
        "max_len = 40  # Most sentences will fit, truncate if needed\n",
        "print(f\"Using max sequence length: {max_len}\")\n",
        "\n",
        "print(\"Padding sequences...\")\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "X = np.array(padded_sequences, dtype=np.int32)\n",
        "y = np.array(labels, dtype=np.int32)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Train/Validation/Test Split (70/15/15)\n",
        "train_split = int(0.7 * len(X))\n",
        "val_split = int(0.85 * len(X))\n",
        "\n",
        "X_train, X_val, X_test = X[:train_split], X[train_split:val_split], X[val_split:]\n",
        "y_train, y_val, y_test = y[:train_split], y[train_split:val_split], y[val_split:]\n",
        "\n",
        "print(f\"\\nVocabulary size: {vocab_size:,}\")\n",
        "print(f\"Training samples: {len(X_train):,}\")\n",
        "print(f\"Validation samples: {len(X_val):,}\")\n",
        "print(f\"Testing samples: {len(X_test):,}\")\n",
        "\n",
        "# RNN Model (will struggle with long sequences)\n",
        "print(\"\\nBuilding RNN model...\")\n",
        "rnn_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "    SimpleRNN(32, activation='tanh'),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# LSTM Model (should handle long-range dependencies better)\n",
        "print(\"Building LSTM model...\")\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "    LSTM(32, activation='tanh'),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL SUMMARIES\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nRNN Model:\")\n",
        "rnn_model.summary()\n",
        "print(\"\\nLSTM Model:\")\n",
        "lstm_model.summary()\n",
        "\n",
        "# Train RNN\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training RNN on 1 Million Samples...\")\n",
        "print(\"=\"*60)\n",
        "rnn_history = rnn_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=5,  # Fewer epochs due to large dataset\n",
        "    batch_size=256,  # Larger batch size for efficiency\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train LSTM\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training LSTM on 1 Million Samples...\")\n",
        "print(\"=\"*60)\n",
        "lstm_history = lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=5,  # Fewer epochs due to large dataset\n",
        "    batch_size=256,  # Larger batch size for efficiency\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL EVALUATION ON TEST SET (150,000 samples)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rnn_loss, rnn_acc = rnn_model.evaluate(X_test, y_test, batch_size=256, verbose=1)\n",
        "lstm_loss, lstm_acc = lstm_model.evaluate(X_test, y_test, batch_size=256, verbose=1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nRNN  - Test Accuracy: {rnn_acc*100:.2f}%, Test Loss: {rnn_loss:.4f}\")\n",
        "print(f\"LSTM - Test Accuracy: {lstm_acc*100:.2f}%, Test Loss: {lstm_loss:.4f}\")\n",
        "print(f\"\\n{'LSTM WINS!' if lstm_acc > rnn_acc else 'RNN WINS!'}\")\n",
        "print(f\"Performance Gap: {abs(lstm_acc - rnn_acc)*100:.2f}% ({'LSTM' if lstm_acc > rnn_acc else 'RNN'} is better)\")\n",
        "\n",
        "# Test on complex sentences\n",
        "test_sentences = [\n",
        "    \"The movie started with brilliant scenes and amazing acting, however after the first hour it became boring and terrible, ultimately leaving me disappointed.\",\n",
        "    \"Initially the film was terrible with boring dialogue, but thankfully the second half became fantastic and the conclusion was beautiful, making it wonderful overall.\",\n",
        "    \"Despite having stunning visuals at the beginning, the movie deteriorated into a confusing mess with slow pacing and forgettable conclusion.\",\n",
        "    \"What started as a tedious slog with poor characters eventually evolved into magnificent brilliance with powerful depth and a spectacular conclusion.\",\n",
        "    \"Although it began with disappointing promise and weak performances, the film remarkably became outstanding with an excellent climax and amazing ending.\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTIONS ON COMPLEX TEST SENTENCES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, test_sentence in enumerate(test_sentences, 1):\n",
        "    test_seq = tokenizer.texts_to_sequences([test_sentence])\n",
        "    test_seq = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    rnn_pred = rnn_model.predict(test_seq, verbose=0)\n",
        "    lstm_pred = lstm_model.predict(test_seq, verbose=0)\n",
        "\n",
        "    # Determine actual label based on sentence structure\n",
        "    actual = \"Negative\" if any(word in test_sentence.lower() for word in [\"disappointed\", \"forgettable\", \"deteriorated\"]) and i <= 3 else \"Positive\"\n",
        "\n",
        "    rnn_result = 'Positive' if rnn_pred[0][0] > 0.5 else 'Negative'\n",
        "    lstm_result = 'Positive' if lstm_pred[0][0] > 0.5 else 'Negative'\n",
        "\n",
        "    print(f\"\\n[Test {i}] {test_sentence[:70]}...\")\n",
        "    print(f\"  Actual:  {actual}\")\n",
        "    print(f\"  RNN:     {rnn_result:8s} (confidence: {rnn_pred[0][0]:.4f}) {'✓' if rnn_result == actual else '✗'}\")\n",
        "    print(f\"  LSTM:    {lstm_result:8s} (confidence: {lstm_pred[0][0]:.4f}) {'✓' if lstm_result == actual else '✗'}\")\n",
        "\n",
        "# Save models\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Saving models...\")\n",
        "rnn_model.save('/home/claude/rnn_sentiment_1M.h5')\n",
        "lstm_model.save('/home/claude/lstm_sentiment_1M.h5')\n",
        "print(\"Models saved successfully!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6yWlH9uUGVS6",
        "outputId": "f4bc8897-2bdb-45c5-95de-81d574d29f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 1 million training samples with long-range dependencies...\n",
            "This may take a few minutes...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating data: 100%|██████████| 10/10 [00:03<00:00,  2.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shuffling data...\n",
            "Total samples generated: 1,000,000\n",
            "\n",
            "Example NEGATIVE (starts positive, ends negative):\n",
            "The opening was spectacular and the characters seemed stirring, yet as the story progressed everything became uninspired and hackneyed, resulting in a dissatisfied experience.\n",
            "\n",
            "Example POSITIVE (starts negative, ends positive):\n",
            "The movie started with disappointing pacing and convoluted dialogue, however the final act was absolutely superb and stirring, leaving me thoroughly overjoyed.\n",
            "\n",
            "Tokenizing sentences...\n",
            "Using max sequence length: 40\n",
            "Padding sequences...\n",
            "\n",
            "Vocabulary size: 216\n",
            "Training samples: 700,000\n",
            "Validation samples: 150,000\n",
            "Testing samples: 150,000\n",
            "\n",
            "Building RNN model...\n",
            "Building LSTM model...\n",
            "\n",
            "============================================================\n",
            "MODEL SUMMARIES\n",
            "============================================================\n",
            "\n",
            "RNN Model:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn_3 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LSTM Model:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training RNN on 1 Million Samples...\n",
            "============================================================\n",
            "Epoch 1/5\n",
            "\u001b[1m2735/2735\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 25ms/step - accuracy: 0.9956 - loss: 0.0241 - val_accuracy: 1.0000 - val_loss: 2.6361e-06\n",
            "Epoch 2/5\n",
            "\u001b[1m2735/2735\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.0492e-06 - val_accuracy: 1.0000 - val_loss: 2.3585e-07\n",
            "Epoch 3/5\n",
            "\u001b[1m2735/2735\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 9.9957e-07 - val_accuracy: 1.0000 - val_loss: 2.9362e-08\n",
            "Epoch 4/5\n",
            "\u001b[1m2735/2735\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.1545e-07 - val_accuracy: 1.0000 - val_loss: 4.1439e-09\n",
            "Epoch 5/5\n",
            "\u001b[1m2735/2735\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 5.4471e-08 - val_accuracy: 1.0000 - val_loss: 5.6968e-10\n",
            "\n",
            "============================================================\n",
            "Training LSTM on 1 Million Samples...\n",
            "============================================================\n",
            "Epoch 1/5\n",
            "\u001b[1m2735/2735\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 64ms/step - accuracy: 0.9723 - loss: 0.0564 - val_accuracy: 1.0000 - val_loss: 1.2011e-06\n",
            "Epoch 2/5\n",
            "\u001b[1m2735/2735\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 4.6757e-06 - val_accuracy: 1.0000 - val_loss: 1.0802e-07\n",
            "Epoch 3/5\n",
            "\u001b[1m2735/2735\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 8.0019e-07 - val_accuracy: 1.0000 - val_loss: 1.3456e-08\n",
            "Epoch 4/5\n",
            "\u001b[1m 675/2735\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 2.8062e-07"
          ]
        }
      ]
    }
  ]
}